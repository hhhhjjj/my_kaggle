{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取数据时候可以用Multiprocessing\n",
    "# 能够减少数据读取消耗时间\n",
    "# import multiprocessing\n",
    "# with multiprocessing.Pool() as pool:\n",
    "#     test_identity, test_transaction, train_identity, train_transaction = pool.map(load_data, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据数据类型的最大值和最小值，我们可以把数据类型改成int8,int16,int32,float32这些\n",
    "# 比如有的数据都是整数，用int就能节省空间\n",
    "def reduce_mem_usage1(props):\n",
    "    # 以下代码补充缺失值为 min-1     start_mem_usg = props.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in.     for col in props.columns:\n",
    "    if props[col].dtype != object:  # Exclude strings \n",
    "        # Print current column type             print(\"******************************\")\n",
    "        print(\"Column: \",col)\n",
    "        print(\"dtype before: \",props[col].dtype)\n",
    "\n",
    "        # make variables for Int, max and min             IsInt = False\n",
    "        mx = props[col].max()\n",
    "        mn = props[col].min()\n",
    "\n",
    "        # Integer does not support NA, therefore, NA needs to be filled             if not np.isfinite(props[col]).all():\n",
    "        NAlist.append(col)\n",
    "        props[col].fillna(mn-1,inplace=True)\n",
    "\n",
    "        # test if column can be converted to an integer             asint = props[col].fillna(0).astype(np.int64)\n",
    "        result = (props[col] - asint)\n",
    "        result = result.sum()\n",
    "        if result > -0.01 and result < 0.01:\n",
    "            IsInt = True\n",
    "\n",
    "\n",
    "        # Make Integer/unsigned Integer datatypes             if IsInt:\n",
    "            if mn >= 0:\n",
    "                if mx < 255:\n",
    "                    props[col] = props[col].astype(np.uint8)\n",
    "                elif mx < 65535:\n",
    "                    props[col] = props[col].astype(np.uint16)\n",
    "                elif mx < 4294967295:\n",
    "                    props[col] = props[col].astype(np.uint32)\n",
    "                else:\n",
    "                    props[col] = props[col].astype(np.uint64)\n",
    "            else:\n",
    "                if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                    props[col] = props[col].astype(np.int8)\n",
    "                elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                    props[col] = props[col].astype(np.int16)\n",
    "                elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                    props[col] = props[col].astype(np.int32)\n",
    "                elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                    props[col] = props[col].astype(np.int64)\n",
    "\n",
    "        # Make float datatypes 32 bit             else:\n",
    "            props[col] = props[col].astype(np.float32)\n",
    "\n",
    "        # Print new column type             print(\"dtype after: \",props[col].dtype)\n",
    "        print(\"******************************\")\n",
    "\n",
    "    # Print final result     print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return props, NAlist\n",
    "\n",
    "# Memory usage of properties dataframe is : 1775.1522827148438 MB # ___MEMORY USAGE AFTER COMPLETION:___ # Memory usage is: 452.7989959716797 MB # This is 25.507614213197968 % of the initial size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA感觉作用还是很大的\n",
    "# 对于分类变量看每个类别不同值的数量\n",
    "# 当然还可以分箱操作，分类变量也可以\n",
    "# 分箱其实就是类似我们原来的分段讨论，每一个段用一段函数，这样子的话效果更佳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比赛上很多匿名特征，但是有的信息是可以挖掘出来的，看运气"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Relaxation\n",
    "# 删除训练集中的所有值出现的评论是测试集中的三倍的值，就是这种值和测试集的差异比较大，没办法预测\n",
    "# 还可以删除出现次数较少的值，或者只在训练集中出现的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建UID\n",
    "# 就是唯一的用户ID\n",
    "# 比如根据信用卡匹配信息，支付卡相关信息和买方邮件域相同的认为是同一个用户\n",
    "columns = ['M'+ str(i) for i in range(1,10)]+['card'+ str(i) for i in range(1,7)]+['P_emaildomain']\n",
    "# identity = pd.concat([train[columns],test[columns]]).drop_duplicates() \n",
    "identity = test[columns].drop_duplicates()\n",
    "identity['identity_id'] = list(range(len(identity)))\n",
    "\n",
    "all_data = pd.concat([train.drop('isFraud', axis=1), test])\n",
    "all_data = all_data.merge(identity, on=columns, how='left')\n",
    "all_data = all_data[~pd.isnull(all_data['identity_id'])]\n",
    "\n",
    "all_data = all_data.groupby('identity_id')['TransactionDT'].agg(['max','min','count']).reset_index()\n",
    "all_data = all_data.sort_values('min',ascending=True).reset_index(drop=True)\n",
    "\n",
    "all_data['percent'] = 1/len(all_data)\n",
    "all_data['percent'] = all_data['percent'].cumsum()\n",
    "\n",
    "all_data[all_data['min']>max(train['TransactionDT'])].shape[0]/all_data.shape[0]\n",
    "# 用UID对特征进行聚合，但是不用这个作为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对抗验证–adversarial validation\n",
    "# 测试集中很多数据肯定是没有出现在训练集中的，所以用这个来判断哪些列可以确定UID\n",
    "# 合并训练集和测试集，并且将训练集和测试集的标签分别设置为0和1；\n",
    "# 构建一个分类器，用于学习the different between testing and training data；\n",
    "# 找到训练集中与测试集最相似的样本（most resemble data），作为验证集，其余的作为训练集；【也可将概率作为样本的权重，做有权重的交叉验证】\n",
    "# 观察AUC，理想的状况是在0.5左右。\n",
    "from xgboost import XGBClassifier\n",
    "import catboost as cbt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for i in ['D' + str(i) for i in range(1,16)]:\n",
    "    train[i] = np.floor(train.TransactionDT / (24*60*60)) - train[i]\n",
    "    test[i] = np.floor(test.TransactionDT / (24*60*60)) - test[i]\n",
    "\n",
    "    features = ['TransactionAmt',\n",
    "           'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "           'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n",
    "           'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
    "           'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n",
    "           'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n",
    "           'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "    all_data = pd.concat([train[features].sample(frac=0.1),\n",
    "               test[features].sample(frac=0.1)])\n",
    "    all_data['is_this_transaction_in_test_data'] = [0]*train.sample(frac=0.1).shape[0] + [1]*test.sample(frac=0.1).shape[0]\n",
    "\n",
    "cat_col = all_data.select_dtypes(object).columns\n",
    "for i in cat_col:\n",
    "    lbl = LabelEncoder()\n",
    "    all_data[i] = lbl.fit_transform(all_data[i].astype(str))\n",
    "\n",
    "cat_list = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "            'P_emaildomain', 'ProductCD', 'R_emaildomain', 'card4', 'card6']\n",
    "cbt_model = cbt.CatBoostClassifier(iterations=1000,learning_rate=0.1,verbose=100,eval_metric='AUC')\n",
    "cbt_model.fit(all_data.drop(['is_this_transaction_in_test_data'],axis=1),all_data['is_this_transaction_in_test_data'])\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_test = cbt_model.predict(all_data.drop(['is_this_transaction_in_test_data'],axis=1))\n",
    "roc_auc_score(y_test, all_data['is_this_transaction_in_test_data'])\n",
    "\n",
    "feature = pd.DataFrame({'importance':cbt_model.feature_importances_, 'feature':cbt_model.feature_names_})\n",
    "feature = feature.sort_values('importance',ascending=False)\n",
    "feature = feature[feature['importance']!=0]\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.barh(feature['feature'],feature['importance'],height =0.5)\n",
    "# 就是画出来看哪两个列相等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['card1', 'addr1', 'D1']\n",
    "all_data = pd.concat([train[features].sample(frac=0.1),\n",
    "           test[features].sample(frac=0.1)])\n",
    "all_data['is_this_transaction_in_test_data'] = [0]*train.sample(frac=0.1).shape[0] + [1]*test.sample(frac=0.1).shape[0]\n",
    "\n",
    "cat_col = all_data.select_dtypes(object).columns\n",
    "for i in cat_col:\n",
    "    lbl = LabelEncoder()\n",
    "    all_data[i] = lbl.fit_transform(all_data[i].astype(str))\n",
    "\n",
    "cat_list = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "            'P_emaildomain', 'ProductCD', 'R_emaildomain', 'card4', 'card6']\n",
    "cbt_model = cbt.CatBoostClassifier(iterations=1000,learning_rate=0.1,verbose=100,eval_metric='AUC')\n",
    "cbt_model.fit(all_data.drop(['is_this_transaction_in_test_data'],axis=1),all_data['is_this_transaction_in_test_data'])\n",
    "\n",
    "y_test = cbt_model.predict(all_data.drop(['is_this_transaction_in_test_data'],axis=1))\n",
    "roc_auc_score(y_test, all_data['is_this_transaction_in_test_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Functions\n",
    "# FREQUENCY ENCODE TOGETHER def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col],df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype('float32')\n",
    "        print(nm,', ',end='')\n",
    "\n",
    "# LABEL ENCODE def encode_LE(col,train=X_train,test=X_test,verbose=True):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True) # Encode the object as an enumerated type or categorical variable.                                              # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html     nm = col\n",
    "    if df_comb.max()>32000:\n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb[len(train):].astype('int16')\n",
    "    del df_comb; x=gc.collect()\n",
    "    if verbose: print(nm,', ',end='')\n",
    "\n",
    "# GROUP AGGREGATION MEAN AND STD def encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test,\n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS     for main_column in main_columns:\n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()\n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "\n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
    "\n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "\n",
    "# COMBINE FEATURES def encode_CB(col1,col2,df1=X_train,df2=X_test):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str)\n",
    "    encode_LE(nm,verbose=False)\n",
    "    print(nm,', ',end='')\n",
    "\n",
    "# GROUP AGGREGATION NUNIQUE def encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n",
    "    for main_column in main_columns:\n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(col+'_'+main_column+'_ct, ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The procedure for engineering features is as follows. First you think of an idea and create a new feature. \n",
    "# Then you add it to your model and evaluate whether local validation AUC increases or decreases. \n",
    "# If AUC increases keep the feature, otherwise discard the feature.\n",
    "# TRANSACTION AMT CENTS # 有很多小数点可能是海淘（汇率转换） X_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\n",
    "X_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\n",
    "print('cents, ', end='')\n",
    "\n",
    "# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN encode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'])\n",
    "\n",
    "# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN encode_CB('card1','addr1')\n",
    "encode_CB('card1_addr1','P_emaildomain')\n",
    "\n",
    "# FREQUENCY ENOCDE encode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n",
    "\n",
    "# GROUP AGGREGATE encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择feature，一般也就是一个个加或者一个个删\n",
    "# 还有就是importance，相关度，时间连续，训练集和测试集分布\n",
    "\n",
    "# permutation importance：\n",
    "# 首先我们有一个已经训练好的模型以及该模型的预测表现（如RMSE），\n",
    "# 比如房价预测模型本来在validation数据上的RMSE是200。然后针对其中的变量（如面积），把这个变量的值全部打乱重新排序，\n",
    "# 用这个重新排序过的数据来做预测，得到一个预测表现。比如说这下RMSE变成了500，那么面积这个变量的重要性就可以记为500-200=300\n",
    "# 这个就是针对单个特征来试\n",
    "\n",
    "# client consistency：\n",
    "# 若样本1与样本2属于不同的分类，但在特征A、 B上的取值完全一样，那么特征子集{A，B}不应该选作最终的特征集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间连续：对于每个特征构建一个模型，用第一个月数据训练，预测最后一个月数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariate Shift协变量转换：检查特征在训练集合测试集的分布是否一致，AUC为0.5的时候，说明分布变化不大\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "\n",
    "def covariate_shift(feature):\n",
    "    df_train = pd.DataFrame(data={feature: train[feature], 'isTest': 0})\n",
    "    df_test = pd.DataFrame(data={feature: test[feature], 'isTest': 1})\n",
    "\n",
    "    # Creating a single dataframe     df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "    # Encoding if feature is categorical     if str(df[feature].dtype) in ['object', 'category']:\n",
    "        df[feature] = LabelEncoder().fit_transform(df[feature].astype(str))\n",
    "\n",
    "    # Splitting it to a training and testing set     X_train, X_test, y_train, y_test = train_test_split(df[feature], df['isTest'], test_size=0.33, random_state=47, stratify=df['isTest'])\n",
    "\n",
    "    clf = lgb.LGBMClassifier(**params, num_boost_round=500)\n",
    "    clf.fit(X_train.values.reshape(-1, 1), y_train)\n",
    "    roc_auc =  roc_auc_score(y_test, clf.predict_proba(X_test.values.reshape(-1, 1))[:, 1])\n",
    "\n",
    "    del df, X_train, y_train, X_test, y_test\n",
    "    gc.collect();\n",
    "\n",
    "    return roc_auc\n",
    "    # 如果输出比较高，就需要删除一部分数据了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAN search\n",
    "# 根据缺失值个数划分block，即当原始特征中缺失值个数都是89个，则它们划分为同一个block\n",
    "nans_df = train.isna()\n",
    "nans_groups={}\n",
    "i_cols = ['V'+str(i) for i in range(1,340)]\n",
    "for col in train.columns:\n",
    "    cur_group = nans_df[col].sum()\n",
    "    try:\n",
    "        nans_groups[cur_group].append(col)\n",
    "    except:\n",
    "        nans_groups[cur_group]=[col]\n",
    "# 每一个block的特征其实是可能有相关性的，有三种方法处理\n",
    "# Applied PCA on each group individually\n",
    "# Selected a maximum sized subset of uncorrelated columns from each group\n",
    "# Replaced the entire group with all columns averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先根据NaN的数量，D11 & V1-V11分为一个block，接着画出V1-V11相关性图：\n",
    "def make_corr(Vs,Vtitle=''):\n",
    "    cols = ['TransactionDT'] + Vs\n",
    "    plt.figure(figsize=(15,15))\n",
    "    sns.heatmap(train[cols].corr(), cmap='RdBu_r', annot=True, center=0.0, fmt=\".2f\")\n",
    "    if Vtitle!='': plt.title(Vtitle,fontsize=14)\n",
    "    else: plt.title(Vs[0]+' - '+Vs[-1],fontsize=14)\n",
    "    plt.show()\n",
    "# 将相关性大于0.75的看为同一组，即D11 & V1-V11可划分为 [[V1],[V2,V3],[V4,V5],[V6,V7],[V8,V9],[V10,V11]]，\n",
    "# 对每个group保留nunique值多大的列\n",
    "grps = [[1],[2,3],[4,5],[6,7],[8,9],[10,11]]\n",
    "def reduce_group(grps,c='V'):\n",
    "    use = []\n",
    "    for g in grps:\n",
    "        mx = 0\n",
    "        vx = g[0]\n",
    "        for gg in g:\n",
    "            n = train[c+str(gg)].nunique()\n",
    "            if n>mx:\n",
    "                mx = n\n",
    "                vx = gg\n",
    "        use.append(vx)    # 保留 subset 中 unique 值最多的列     print('Use these',use)\n",
    "reduce_group(grps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要相信单独一个validation，我们可以构造多个validation：使用前四个月训练，跳过一个月，预测最后一个月；前两个月训练，跳过2个月，预测最后一个月；\n",
    "# 第一个月训练，跳过三个月，预测最后一个月。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了防止过拟合，不能直接使用 UID，因为在测试集中有 60% 多的新用户。但是可以根据UID来提取特征：\n",
    "new_features = df.groupby('uid')[columns].agg(['mean'])\n",
    "# 这样模型就有能力来识别未看过的用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing\n",
    "# 因为同一个人的所有交易都是isFraud = 0或全部isFraud = 1。\n",
    "# 换句话说，它们的所有预测都是相同的。因此，我们的后期处理是将一个客户的所有预测替换为它们的平均预测，包括训数据集中的isFraud值。\n",
    "X_test['isFraud'] = sample_submission.isFraud.values\n",
    "X_train['isFraud'] = y_train.values\n",
    "comb = pd.concat([X_train[['isFraud']],X_test[['isFraud']]],axis=0)\n",
    "\n",
    "uids = pd.read_csv('data/uids_v4_no_multiuid_cleaning..csv',usecols=['TransactionID','uid']).rename({'uid':'uid2'},axis=1)\n",
    "comb = comb.merge(uids,on='TransactionID',how='left')\n",
    "mp = comb.groupby('uid2').isFraud.agg(['mean'])\n",
    "comb.loc[comb.uid2>0,'isFraud'] = comb.loc[comb.uid2>0].uid2.map(mp['mean'])\n",
    "\n",
    "uids = pd.read_csv('data/uids_v1_no_multiuid_cleaning.csv',usecols=['TransactionID','uid']).rename({'uid':'uid3'},axis=1)\n",
    "comb = comb.merge(uids,on='TransactionID',how='left')\n",
    "mp = comb.groupby('uid3').isFraud.agg(['mean'])\n",
    "comb.loc[comb.uid3>0,'isFraud'] = comb.loc[comb.uid3>0].uid3.map(mp['mean'])\n",
    "\n",
    "sample_submission.isFraud = comb.iloc[len(X_train):].isFraud.values\n",
    "sample_submission.to_csv('sub_xgb_96_PP.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
