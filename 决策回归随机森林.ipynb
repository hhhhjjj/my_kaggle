{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# 表示进这个源代码里面只能看到各个参数的说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calEnt(dataSet):\n",
    "    n = dataSet.shape[0]                             #数据集总行数\n",
    "    iset = dataSet.iloc[:,-1].value_counts()         #标签的所有类别\n",
    "    p = iset/n                                       #每一类标签所占比\n",
    "    ent = (-p*np.log2(p)).sum()                      #计算信息熵\n",
    "    return ent\n",
    "# 这个计算香农熵其实也就是计算信息熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 然后就计算信息增益\n",
    "# 信息增益等于信息熵减去每个列对应的信息熵\n",
    "# 把所有列的信息熵都给算出来\n",
    "# 注意这里每一行是特征，列选择其实是选择分割点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择最优的列进行切分\n",
    "def bestSplit(dataSet):\n",
    "    baseEnt = calEnt(dataSet)                                #计算原始熵\n",
    "    bestGain = 0                                             #初始化信息增益\n",
    "    axis = -1                                                #初始化最佳切分列，标签列\n",
    "    for i in range(dataSet.shape[1]-1):                      #对特征的每一列进行循环\n",
    "        levels= dataSet.iloc[:,i].value_counts().index       #提取出当前列的所有取值\n",
    "        ents = 0                                             #初始化子节点的信息熵       \n",
    "        for j in levels:                                     #对当前列的每一个取值进行循环\n",
    "            childSet = dataSet[dataSet.iloc[:,i]==j]         #某一个子节点的dataframe\n",
    "            ent = calEnt(childSet)                           #计算某一个子节点的信息熵\n",
    "            ents += (childSet.shape[0]/dataSet.shape[0])*ent #计算当前列的信息熵\n",
    "        #print(f'第{i}列的信息熵为{ents}')\n",
    "        infoGain = baseEnt-ents                              #计算当前列的信息增益\n",
    "        #print(f'第{i}列的信息增益为{infoGain}')\n",
    "        if (infoGain > bestGain):\n",
    "            bestGain = infoGain                              #选择最大信息增益\n",
    "            axis = i                                         #最大信息增益所在列的索引\n",
    "    return axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到列之后我们就可以按照给定的列切分数据集\n",
    "\"\"\"\n",
    "函数功能：按照给定的列划分数据集\n",
    "参数说明：\n",
    "    dataSet：原始数据集\n",
    "    axis：指定的列索引\n",
    "    value：指定的属性值\n",
    "返回：\n",
    "    redataSet：按照指定列索引和属性值切分后的数据集\n",
    "\"\"\"\n",
    "\n",
    "def mySplit(dataSet,axis,value):\n",
    "    col = dataSet.columns[axis]\n",
    "    redataSet = dataSet.loc[dataSet[col]==value,:].drop(col,axis=1)\n",
    "    return redataSet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能：基于最大信息增益切分数据集，递归构建决策树\n",
    "参数说明：\n",
    "    dataSet：原始数据集（最后一列是标签）\n",
    "返回：\n",
    "    myTree：字典形式的树\n",
    "\"\"\"\n",
    "def createTree(dataSet):\n",
    "    featlist = list(dataSet.columns)                          #提取出数据集所有的列\n",
    "    classlist = dataSet.iloc[:,-1].value_counts()             #获取最后一列类标签\n",
    "    #判断最多标签数目是否等于数据集行数，或者数据集是否只有一列\n",
    "    if classlist[0]==dataSet.shape[0] or dataSet.shape[1] == 1:\n",
    "        return classlist.index[0]                             #如果是，返回类标签\n",
    "    axis = bestSplit(dataSet)                                 #确定出当前最佳切分列的索引\n",
    "    bestfeat = featlist[axis]                                 #获取该索引对应的特征\n",
    "    myTree = {bestfeat:{}}                                    #采用字典嵌套的方式存储树信息\n",
    "    del featlist[axis]                                        #删除当前特征\n",
    "    valuelist = set(dataSet.iloc[:,axis])                     #提取最佳切分列所有属性值\n",
    "    for value in valuelist:                                   #对每一个属性值递归建树\n",
    "        myTree[bestfeat][value] = createTree(mySplit(dataSet,axis,value))\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以后就不要浪费时间在构建决策树上面了\n",
    "#树的存储\n",
    "np.save('myTree.npy',myTree)\n",
    "\n",
    "#树的读取\n",
    "read_myTree = np.load('myTree.npy').item()   \n",
    "read_myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能：对一个测试实例进行分类\n",
    "参数说明：\n",
    "    inputTree：已经生成的决策树\n",
    "    labels：存储选择的最优特征标签\n",
    "    testVec：测试数据列表，顺序对应原数据集\n",
    "返回：\n",
    "    classLabel：分类结果\n",
    "\"\"\"\n",
    "def classify(inputTree,labels, testVec):\n",
    "    firstStr = next(iter(inputTree))                   #获取决策树第一个节点\n",
    "    secondDict = inputTree[firstStr]                   #下一个字典\n",
    "    featIndex = labels.index(firstStr)                 #第一个节点所在列的索引\n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featIndex] == key:\n",
    "            if type(secondDict[key]) == dict :\n",
    "                classLabel = classify(secondDict[key], labels, testVec)\n",
    "            else: \n",
    "                classLabel = secondDict[key]\n",
    "    return classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能：对测试集进行预测，并返回预测后的结果\n",
    "参数说明：\n",
    "    train：训练集\n",
    "    test：测试集\n",
    "返回：\n",
    "    test：预测好分类的测试集\n",
    "\"\"\"\n",
    "def acc_classify(train,test):\n",
    "    inputTree = createTree(train)                       #根据测试集生成一棵树\n",
    "    labels = list(train.columns)                        #数据集所有的列名称\n",
    "    result = []\n",
    "    for i in range(test.shape[0]):                      #对测试集中每一条数据进行循环\n",
    "        testVec = test.iloc[i,:-1]                      #测试集中的一个实例\n",
    "        classLabel = classify(inputTree,labels,testVec) #预测该实例的分类\n",
    "        result.append(classLabel)                       #将分类结果追加到result列表中\n",
    "    test['predict']=result                              #将预测结果追加到测试集最后一列\n",
    "    acc = (test.iloc[:,-1]==test.iloc[:,-2]).mean()     #计算准确率\n",
    "    print(f'模型预测准确率为{acc}')\n",
    "    return test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回归树预测年龄这些连续数据\n",
    "# 之前的决策树用的是信息熵，现在回归树用最小均方误差就行\n",
    "# 然后遍历特征某一列所有不重复的点，找MSE最小的点作为最佳分割点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林就是多个决策树结果用投票来选最好\n",
    "# 原始样本有放回抽样k次，训练k个模型来投票，bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boosting就是每个样本建立权重，每一轮训练集不变，当某个样本被误分类概率高的时候，加大权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
