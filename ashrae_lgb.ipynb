{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data15070\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. All changes under this directory will be kept even after reset. Please clean unnecessary files in time to speed up environment loading.\n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: seaborn in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: pandas>=0.15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seaborn) (0.23.4)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seaborn) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.9.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seaborn) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas>=0.15.2->seaborn) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas>=0.15.2->seaborn) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (2.4.2)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (1.12.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (41.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: lightgbm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.3.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from lightgbm) (0.20.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from lightgbm) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from lightgbm) (1.16.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\r\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import gc, math\r\n",
    "\r\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "import lightgbm as lgb\r\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\r\n",
    "from tqdm import tqdm\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "sns.set(rc={'figure.figsize':(11,8)})\r\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\r\n",
    "metadata_df = pd.read_csv('/home/aistudio/data/data15070/building_m.csv')\r\n",
    "train_df = pd.read_csv('/home/aistudio/data/data15070/train.csv', parse_dates=['timestamp'])\r\n",
    "# 这个parse_dates的意思就是将csv中的时间字符串转换成日期格式\r\n",
    "test_df = pd.read_csv('/home/aistudio/data/data15070/test.csv', parse_dates=['timestamp'])\r\n",
    "weather_train_df = pd.read_csv('/home/aistudio/data/data15070/w_train.csv', parse_dates=['timestamp'])\r\n",
    "weather_test_df = pd.read_csv('/home/aistudio/data/data15070/w_test.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这个比赛是有时间戳的，但是这个时间戳还对不上，所以我们需要先对齐时间\n",
    "weather = pd.concat([weather_train_df,weather_test_df],ignore_index=True)\n",
    "weather_key = ['site_id', 'timestamp']\n",
    "\n",
    "temp_skeleton = weather[weather_key + ['air_temperature']].drop_duplicates(subset=weather_key).sort_values(by=weather_key).copy()\n",
    "# subset这个的意思是去掉重复行就考虑weather_key这个\n",
    "temp_skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate ranks of hourly temperatures within date/site_id chunks\n",
    "temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')\n",
    "# groupby就是拆分应用合并\n",
    "# 先从原来的临时框架中拆分出site_id和timestamp的日部分，然后从这里面看温度，根据平均进行排序\n",
    "# 就是对site_id和日期对每小时温度排序\n",
    "temp_skeleton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)\n",
    "df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n",
    "# unstack将聚合的两列一列值为行索引，另外一列为列索引\n",
    "\n",
    "df_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\n",
    "# 用温度峰值-14，得到时间戳对齐间隔\n",
    "site_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\n",
    "site_ids_offsets.index.name = 'site_id'\n",
    "\n",
    "def timestamp_align(df):\n",
    "    df['offset'] = df.site_id.map(site_ids_offsets)\n",
    "    df['timestamp_aligned'] = (df.timestamp - pd.to_timedelta(df.offset, unit='H'))\n",
    "    df['timestamp'] = df['timestamp_aligned']\n",
    "    del df['timestamp_aligned']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_train_df = timestamp_align(weather_train_df)\r\n",
    "weather_test_df = timestamp_align(weather_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del weather \r\n",
    "del df_2d\r\n",
    "del temp_skeleton\r\n",
    "del site_ids_offsets\r\n",
    "weather_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_train_df = weather_train_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))\n",
    "# 先对site_id分组，然后对每个单独的进行apply操作\n",
    "# 这个操作就是通过前后数据插值来填充NAN\n",
    "# 这个both就是说处理前面和后面的nan\n",
    "weather_test_df = weather_test_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))\n",
    "weather_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_train_df.isna().sum()\n",
    "# 没想到还是有缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['meter_reading'] = np.log1p(train_df['meter_reading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function to reduce the memory usage\r\n",
    "def reduce_mem_usage(df, verbose=True):\r\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\r\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \r\n",
    "    for col in df.columns:\r\n",
    "        col_type = df[col].dtypes\r\n",
    "        if col_type in numerics:\r\n",
    "            c_min = df[col].min()\r\n",
    "            c_max = df[col].max()\r\n",
    "            if str(col_type)[:3] == 'int':\r\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\r\n",
    "                    df[col] = df[col].astype(np.int8)\r\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\r\n",
    "                    df[col] = df[col].astype(np.int16)\r\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\r\n",
    "                    df[col] = df[col].astype(np.int32)\r\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\r\n",
    "                    df[col] = df[col].astype(np.int64)  \r\n",
    "            else:\r\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\r\n",
    "                    df[col] = df[col].astype(np.float16)\r\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\r\n",
    "                    df[col] = df[col].astype(np.float32)\r\n",
    "                else:\r\n",
    "                    df[col] = df[col].astype(np.float64)    \r\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\r\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\r\n",
    "# 这个是标准化标签，将标签值统一转换成rang（标签值个数-1）范围内的，反正感觉就是one-hot加归一化\r\n",
    "metadata_df['primary_use'] = le.fit_transform(metadata_df['primary_use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metadata_df = reduce_mem_usage(metadata_df)\r\n",
    "train_df = reduce_mem_usage(train_df)\r\n",
    "test_df = reduce_mem_usage(test_df)\r\n",
    "weather_train_df = reduce_mem_usage(weather_train_df)\r\n",
    "weather_test_df = reduce_mem_usage(weather_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\r\n",
    "full_train_df = train_df.merge(metadata_df, on='building_id', how='left')\r\n",
    "full_train_df = full_train_df.merge(weather_train_df, on=['site_id', 'timestamp'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete unnecessary dataframes to decrease memory usage\r\n",
    "del train_df\r\n",
    "del weather_train_df\r\n",
    "gc.collect()\r\n",
    "# 这个的作用是清理内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\r\n",
    "# full_test_df = test_df.merge(metadata_df, on='building_id', how='left')\r\n",
    "# full_test_df = full_test_df.merge(weather_test_df, on=['site_id', 'timestamp'], how='left')\r\n",
    "full_test_df = test_df.merge(metadata_df, on='building_id', how='left')\r\n",
    "full_test_df = full_test_df.merge(weather_test_df, on=['site_id', 'timestamp'], how='left')\r\n",
    "del metadata_df\r\n",
    "del weather_test_df\r\n",
    "del test_df\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 把数据处理完了开始EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(pd.unique(full_train_df['primary_use']), full_train_df['primary_use'].value_counts())\r\n",
    "# y轴是数据出现的频率\r\n",
    "ax.set(xlabel='Primary Usage', ylabel='# of records', title='Primary Usage vs. # of records')\r\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=50, ha=\"right\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meter_types = {0: 'electricity', 1: 'chilledwater', 2: 'steam', 3: 'hotwater'}\r\n",
    "ax = sns.barplot(np.vectorize(meter_types.get)(pd.unique(full_train_df['meter'])), full_train_df['meter'].value_counts())\r\n",
    "# 这个np.vectorize就是将函数向量化，当然在这好像就是加快速度用的\r\n",
    "ax.set(xlabel='Meter Type', ylabel='# of records', title='Meter type vs. # of records')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(np.vectorize(meter_types.get)(full_train_df.groupby(['meter'])['meter_reading'].mean().keys()), full_train_df.groupby(['meter'])['meter_reading'].mean())\r\n",
    "\r\n",
    "ax.set(xlabel='Meter Type', ylabel='Meter reading', title='Meter type vs. Meter Reading')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(14, 6))\r\n",
    "ax.set(xlabel='Year Built', ylabel='# Of Buildings', title='Buildings built in each year')\r\n",
    "full_train_df['year_built'].value_counts(dropna=False).sort_index().plot(ax=ax)\r\n",
    "full_test_df['year_built'].value_counts(dropna=False).sort_index().plot(ax=ax)\r\n",
    "ax.legend(['Train', 'Test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开始特征工程\n",
    "pd.DataFrame(full_train_df.isna().sum().sort_values(ascending=False), columns=['NaN Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 然后把这些缺失值都给处理了\n",
    "def mean_without_overflow_fast(col):\n",
    "    col /= len(col)\n",
    "    return col.mean() * len(col)\n",
    "missing_values = (100-full_train_df.count() / len(full_train_df) * 100).sort_values(ascending=False)\n",
    "missing_features = full_train_df.loc[:, missing_values > 0.0]\n",
    "missing_features = missing_features.apply(mean_without_overflow_fast)\n",
    "for key in full_train_df.loc[:, missing_values > 0.0].keys():\n",
    "    if key == 'year_built' or key == 'floor_count':\n",
    "        full_train_df[key].fillna(math.floor(missing_features[key]), inplace=True)\n",
    "        full_test_df[key].fillna(math.floor(missing_features[key]), inplace=True)\n",
    "    else:\n",
    "        full_train_df[key].fillna(missing_features[key], inplace=True)\n",
    "        full_test_df[key].fillna(missing_features[key], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_train_df['timestamp'].dtype\n",
    "# 返回数组中元素的数据类型，这个类型我都不知道是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_train_df['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_train_df[\"timestamp\"] = pd.to_datetime(full_train_df[\"timestamp\"])\r\n",
    "full_test_df[\"timestamp\"] = pd.to_datetime(full_test_df[\"timestamp\"])\r\n",
    "# 表示我实在没看出来这转换完有什么区别，这个函数就是将时间转换成可读的时间\r\n",
    "full_train_df[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform(df):\r\n",
    "    df['hour'] = np.uint8(df['timestamp'].dt.hour)\r\n",
    "    df['day'] = np.uint8(df['timestamp'].dt.day)\r\n",
    "    df['weekday'] = np.uint8(df['timestamp'].dt.weekday)\r\n",
    "    df['month'] = np.uint8(df['timestamp'].dt.month)\r\n",
    "    df['year'] = np.uint8(df['timestamp'].dt.year-1900)\r\n",
    "    \r\n",
    "    df['square_feet'] = np.log(df['square_feet'])\r\n",
    "    \r\n",
    "    return df\r\n",
    "full_train_df = transform(full_train_df)\r\n",
    "full_test_df = transform(full_test_df)\r\n",
    "dates_range = pd.date_range(start='2015-12-31', end='2019-01-01')\r\n",
    "us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\r\n",
    "full_train_df['is_holiday'] = (full_train_df['timestamp'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\r\n",
    "full_test_df['is_holiday'] = (full_test_df['timestamp'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming 5 days a week for all the given buildings\r\n",
    "full_train_df.loc[(full_train_df['weekday'] == 5) | (full_train_df['weekday'] == 6) , 'is_holiday'] = 1\r\n",
    "full_test_df.loc[(full_test_df['weekday']) == 5 | (full_test_df['weekday'] == 6) , 'is_holiday'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这个数据在5月20日之前看起来都是可疑的。所以把这些给删了\n",
    "full_train_df = full_train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_test_df = full_test_df.drop(['timestamp'], axis=1)\r\n",
    "full_train_df = full_train_df.drop(['timestamp'], axis=1)\r\n",
    "print (f'Shape of training dataset: {full_train_df.shape}')\r\n",
    "print (f'Shape of testing dataset: {full_test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Reducing memory\r\n",
    "full_train_df = reduce_mem_usage(full_train_df)\r\n",
    "full_test_df = reduce_mem_usage(full_test_df)\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这个就是风向进行转换了\r\n",
    "def degToCompass(num):\r\n",
    "    val=int((num/22.5)+.5)\r\n",
    "    arr=[i for i in range(0,16)]\r\n",
    "    return arr[(val % 16)]\r\n",
    "beaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \r\n",
    "          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\r\n",
    "\r\n",
    "for item in beaufort:\r\n",
    "    full_train_df.loc[(full_train_df['wind_speed']>=item[1]) & (full_train_df['wind_speed']<item[2]), 'beaufort_scale'] = item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categoricals = ['site_id', 'building_id', 'primary_use', 'hour', 'weekday', 'meter',  'wind_direction', 'is_holiday']\r\n",
    "drop_cols = ['sea_level_pressure', 'wind_speed']\r\n",
    "numericals = ['square_feet', 'year_built', 'air_temperature', 'cloud_coverage',\r\n",
    "              'dew_temperature', 'precip_depth_1_hr', 'floor_count', 'beaufort_scale']\r\n",
    "\r\n",
    "feat_cols = categoricals + numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_train_df = reduce_mem_usage(full_train_df)\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = full_train_df[\"meter_reading\"]\r\n",
    "del full_train_df[\"meter_reading\"]\r\n",
    "full_train_df.drop(drop_cols, axis=1)\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the testing dataset to freeup the RAM. We'll read after training\r\n",
    "# 感觉这个老外很节省内存啊\r\n",
    "full_test_df.to_pickle('/home/aistudio/work/full_test_df.pkl')\r\n",
    "del full_test_df\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\r\n",
    "            'boosting_type': 'gbdt',\r\n",
    "            'objective': 'regression',\r\n",
    "            'metric': {'rmse'},\r\n",
    "            'subsample': 0.4,\r\n",
    "            'subsample_freq': 1,\r\n",
    "            'learning_rate': 0.25,\r\n",
    "            'num_leaves': 40,\r\n",
    "            'feature_fraction': 0.75,\r\n",
    "            'lambda_l1': 1,\r\n",
    "            'lambda_l2': 1\r\n",
    "            }\r\n",
    "\r\n",
    "folds = 2\r\n",
    "seed = 666\r\n",
    "\r\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\r\n",
    "\r\n",
    "models = []\r\n",
    "for train_index, val_index in kf.split(full_train_df, full_train_df['building_id']):\r\n",
    "    train_X = full_train_df[feat_cols].iloc[train_index]\r\n",
    "    val_X = full_train_df[feat_cols].iloc[val_index]\r\n",
    "    train_y = target.iloc[train_index]\r\n",
    "    val_y = target.iloc[val_index]\r\n",
    "    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\r\n",
    "    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\r\n",
    "    gbm = lgb.train(params,\r\n",
    "                lgb_train,\r\n",
    "                num_boost_round=500,\r\n",
    "                valid_sets=(lgb_train, lgb_eval),\r\n",
    "                early_stopping_rounds=100,\r\n",
    "                verbose_eval = 100)\r\n",
    "    models.append(gbm)\r\n",
    "del full_train_df, train_X, val_X, lgb_train, lgb_eval, train_y, val_y, target\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_test_df = pd.read_pickle('/home/aistudio/work/full_test_df.pkl')\n",
    "full_test_df['wind_direction'] = full_test_df['wind_direction'].apply(degToCompass)\n",
    "for item in beaufort:\n",
    "    full_test_df.loc[(full_test_df['wind_speed']>=item[1]) & (full_test_df['wind_speed']<item[2]), 'beaufort_scale'] = item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_test_df = full_test_df[feat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=0\r\n",
    "res=[]\r\n",
    "step_size = 50000\r\n",
    "for j in tqdm(range(int(np.ceil(full_test_df.shape[0]/50000)))):\r\n",
    "    res.append(np.expm1(sum([model.predict(full_test_df.iloc[i:i+step_size]) for model in models])/folds))\r\n",
    "    i+=step_size\r\n",
    "res = np.concatenate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/data15070/sample_s.csv')\r\n",
    "submission['meter_reading'] = res\r\n",
    "submission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\r\n",
    "submission.to_csv('/home/aistudio/work/submission_fe_lgbm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.5.1 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
